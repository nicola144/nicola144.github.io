# Improved Auxiliary Particle Filters

In this post, my aim is to introduce a recent technique called IAPF (Improved Auxiliary Particle Filter). 
The ideal target reader has familiarity with Bayesian inference and basics of particle filters. However, the latter is much more optional, and if you are familiar with Bayesian inference in a "batch" setting (where data is processed all at once), you should be able to follow. If you are not, I will write a blogpost on Bayesian inference that assumes no prior background except basic rules of probability. Even then, I ambitiously hope that this post can be interesting to both Bayesian statistics experts who aren't aware of the work I will describe *and* people who see particle filters for the first time. It's possible, I just have to do it. 

1. [Preliminaries](#introduction)
2. [Particle Filters](#paragraph1)

dio
## Brief introduction to sequential inference 

In Bayesian inference we want to update our beliefs on the state of some random variables, which could represent parameters of a parametric statistical model or represent some unobserved data generating process. Focussing on the "updating" perspective, the step to using Bayesian methods to represent dynamical systems is quite natural. The field of statistical signal processing has been using the rule of probabilities to model object tracking, navigation and even.. spread of infectious diseases. 
The probabilistic evolution of a dynamical system is often called a *state space model*. This is just an abstraction of how we think the state of the system evolves over time. Imagine we are tracking a robot's position (x,y coordinates) and bearing: these constitute a 3 element vector. At some specific timestep, we can have a belief, i.e. a probability distribution that represents how likely we think the robot is currently assuming a certain bearing etc. If we start with a prior, and define some likelihood function/ sampling process that we believe generates what we observe, we can update our belief over the system's state with the rules of probabilty.
Let the (uknown) state of the system at time $t$ be the vector valued random variable $\mathbf{s}_{t}$.

We observe this state through a (noisy) measurement $\mathbf{v}_{t}$ (where v stands for visible). 

Now we have to start making more assumptions. What does our belief on $\mathbf{s}_{t}$ depend on ? 

Suprisingly to me, it turns out for **a lot** of applications it just needs to depend on the state at the previous timestep. 
In other words, we can say that $\mathbf{s}_{t}$ is sampled from some density $f$ conditional on $\mathbf{s}_{t-1}$:


$$
\mathbf{s}_{t} \sim f(\mathbf{s}_{t} \mid \mathbf{s}_{t-1})
$$

Further, usually the observation or visible is sampled according to the current state:

$$
\mathbf{v}_{t} \sim f(\mathbf{v}_{t} \mid \mathbf{s}_{t})
$$

For example, a classic Gaussian likelihood for would imply that the belief over $\mathbf{v}_{t}$ is a linear combination of the current state's features. It is reasonable to assume this: if we take a measurement, we don't expect its outcome to be dependent on previous states of the system, just the current one. 

These collection of random variables and densities define the state space model completely. It is worth if you see this for the first time reflecting on the particular assumptions we are making. How the belief on $\mathbf{s}$ evolves with time could depend on many previous states; the measurement could depend on previous measurements if we had a sensor that degrades over time, etc. I am not great at giving practical examples, but if you are reading this, you should be able to see that this can be generalized in several ways. 
Note that a lot of the structure comes from assuming some variables are statistically independent from others. The field of probabilistic graphical models is dedicated to representing statistical independencies in the form of graphs (nodes and edges). One benefit of the graphical representation is that it makes immediately clear how more flexible we could be. 

In short, when the transition density and the observation densities are linear combination of their inputs with additive, i.i.d. Gaussian noise , then the state space model is often called Linear Dynamical System (LDS). When variables are discrete, it is often called Hidden Markov Model (HMM). These are just labels. 

There are several tasks that we can perform on the state space model described above. Each of these has a fancy name, but you should recall that technically all we are doing is applying the sum and product rule. These tasks are just associated to a *target* distribution which is the object of interest that we want to compute. 

Filtering: The target distributions are of the form: $p\left(\mathbf{s}_{t} | \mathbf{v}_{1: t}\right), \quad t=1, \ldots, T$. This represents what we have learnt about the system's state at time $t$, after observations up to $t$.

Smoothing: The target distributions are of the form: $p\left(\mathbf{s}_{t} | \mathbf{v}_{1: T}\right), \quad t=1, \ldots, T$. This represent what we have learnt about the system's state after observing the *complete* sequence of measurements, and revised the previous beliefs obtained by filtering. 

Parameter Estimation The target distributions are of the form: $p\left(\boldsymbol{\theta} | \mathbf{v}_{1: T}\right)= $.  $\int p\left(\mathbf{s}_{0: T}, \boldsymbol{\theta} | \mathbf{v}_{1: T}\right) \mathrm{d} \mathbf{s}_{0: T}$. In the case that the transition and/or observation densities are parametric and parameters are unknown, we can learn them from data by choosing those that both explain the observations well and agree with our prior beliefs. This task is sometimes referred to as *learning*, because parameters describe properties of sensors that can be estimated from data with machine learning methods. No, it is called learning just because it is cool. 




