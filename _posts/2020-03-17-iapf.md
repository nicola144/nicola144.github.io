# Improved Auxiliary Particle Filters

In this post, my aims are: 
* Introduce Bayesian inference in state space models
* Introduce approximate inference using importance sampling, in state space models
* Finally, describe the Auxiliary Particle Filter, its intepretation and the recent Improved Particle Filter by Elvira et al. [1]

The ideal target reader has familiarity with Bayesian inference, basics of Monte Carlo and importance sampling, basics of particle filters. However, if you are familiar with Bayesian inference in a "batch" setting (where data is processed all at once), you should be able to follow. If you are not, I will write a blogpost on Bayesian inference that assumes no prior background except basic rules of probability. Even then, I ambitiously hope that this post can be interesting to both Bayesian statistics experts who aren't aware of the work I will describe (these can skip to the sections on Auxiliary Particle Filters and Improved Auxiliary Particle Filters.) *and* people who see particle filters for the first time. Ah, knowing some CS may help too. 

1. [Brief introduction to sequential inference](#introduction)
    1. [General Bayesian Filtering](#sub1)
    2. [Recursive Formulations](#sub2)
2. [Particle Filtering](#paragraph1)
3. [The Auxiliary Particle Filter and its interpretations](#paragraph2)
4. [The Improved Particle Filter through the Multiple Importance Sampling Interpretation](#paragraph3)


## Brief introduction to sequential inference <a name="introduction"></a>

In Bayesian inference we want to update our beliefs on the state of some random variables, which could represent parameters of a parametric statistical model or represent some unobserved data generating process. Focussing on the "updating" perspective, the step to using Bayesian methods to represent dynamical systems is quite natural. The field of statistical signal processing has been using the rule of probabilities to model object tracking, navigation and even.. spread of infectious diseases. 
The probabilistic evolution of a dynamical system is often called a *state space model*. This is just an abstraction of how we think the state of the system evolves over time. Imagine we are tracking a robot's position (x,y coordinates) and bearing: these constitute a 3 element vector. At some specific timestep, we can have a belief, i.e. a probability distribution that represents how likely we think the robot is currently assuming a certain bearing etc. If we start with a prior, and define some likelihood function/ sampling process that we believe generates what we observe, we can update our belief over the system's state with the rules of probabilty.
Let the (uknown) state of the system at time $t$ be the vector valued random variable $\mathbf{s}_{t}$.

We observe this state through a (noisy) measurement $\mathbf{v}_{t}$ (where v stands for visible). 

Now we have to start making more assumptions. What does our belief on $\mathbf{s}_{t}$ depend on ? 

Suprisingly to me, it turns out for **a lot** of applications it just needs to depend on the state at the previous timestep. 
In other words, we can say that $$\mathbf{s}_{t}$$ is sampled from some density $$f$$ conditional on $$\mathbf{s}_{t-1}$$:

$$
\color{blue}{\text{Transition density}}: \qquad \mathbf{s}_{t} \sim \color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \tag{1}\label{eq1}
$$

Further, usually the observation or visible is sampled according to the current state:

$$
\color{green}{\text{Observation density}}: \mathbf{v}_{t} \sim \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t}) \tag{2}\label{eq2}
$$

It is reasonable to assume this: if we take a measurement, we don't expect its outcome to be dependent on previous states of the system, just the current one ($$\color{blue}{f}$$ and $$\color{green}{g}$$ seem arbitrary but they are common in the literature). For example, a classic Gaussian likelihood for would imply that the belief over $$\mathbf{v}_{t}$$ is a Normal with a mean being a linear combination of the state's coordinates.

These collection of random variables and densities define the state space model completely. It is worth, if you see this for the first time, reflecting on the particular assumptions we are making. How the belief on $\mathbf{s}$ evolves with time could depend on many previous states; the measurement could depend on previous measurements, if we had a sensor that degrades over time, etc... I am not great at giving practical examples, but if you are reading this, you should be able to see that this can be generalized in several ways. 
Note that a lot of the structure comes from assuming some variables are statistically independent from others. The field of probabilistic graphical models is dedicated to representing statistical independencies in the form of graphs (nodes and edges). One benefit of the graphical representation is that it makes immediately clear how flexible we could be. 

In short, when the transition density and the observation densities are linear combination of their inputs with additive, i.i.d. Gaussian noise, then the state space model is often called a Linear Dynamical System (LDS). When variables are discrete, it is often called Hidden Markov Model (HMM). These are just labels. 

There are several tasks that we can perform on the state space model described above. Each of these has a fancy name, but you should recall that technically all we are doing is applying the sum and product rules. These tasks are just associated to a *target* distribution which is the object of interest that we want to compute. Let's list some of these: 

<p>
  <b>Filtering</b>: The target distributions are of the form: $p\left(\mathbf{s}_{t} | \mathbf{v}_{1: t}\right), \quad t=1, \ldots, T$. This represents what we have learnt about the system's state at time $t$, after observations up to $t$.<br>

  <b>Smoothing</b>:  The target distributions are of the form: $p\left(\mathbf{s}_{t} | \mathbf{v}_{1: T}\right), \quad t=1, \ldots, T$. This represent what we have learnt about the system's state after observing the *complete* sequence of measurements, and revised the previous beliefs obtained by filtering. <br>

<b>Parameter Estimation</b>: The target distributions are of the form: $p\left(\boldsymbol{\theta} | \mathbf{v}_{1: T}\right)= \int p\left(\mathbf{s}_{0: T}, \boldsymbol{\theta} | \mathbf{v}_{1: T}\right) \mathrm{d} \mathbf{s}_{0: T}$. The parameters $\boldsymbol{\theta}$ represent all the parameters of any parametric densities in the state space model. In the case that the transition and/or observation densities are parametric, and parameters are unknown, we can learn them from data by choosing those that both explain the observations well and also agree with our prior beliefs. Parameter estimation is sometimes referred to as *learning*, because parameters describe properties of sensors that can be estimated from data with machine learning methods. So, it is called learning just because it is cool. <br>
</p>

![](../images/hmm.png)

### General Bayesian Filtering <a name="sub1"></a>

#### Some notation
- The notation $$\mathbf{v}_{1:t}$$ means a collection of vectors $$ \left \{ \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_t \right \}$$
- Therefore, $$ p\left ( \mathbf{v}_{1:t} \right )$$ is a joint distribution: $$p\left ( \mathbf{v}_1, \mathbf{v}_2, \dots, \mathbf{v}_t \right ) $$
- Integrating $$ \int p(\mathbf{x}_{1:t}) \mathrm{d}\mathbf{x}_{i:j}$$ means $$ \underbrace{\int \dots \int}_{j-i+1} p(\mathbf{x}_{1:t}) \mathrm{d}\mathbf{x}_{i} \mathrm{d}\mathbf{x}_{i+1} \dots \mathrm{d}\mathbf{x}_{j} $$

In this post, I am only concerned with filtering, and will always assume that any parameters of <span style="color:blue">transition</span> or <span style="color:green">observation</span> densities are known in advance. 
Let's derive how to find the filtering distribution in the state space model described without many assumption on the densities.
Recall that the aim is to compute: $$ p\left(\mathbf{s}_{t} | \mathbf{v}_{1: t}\right)$$. Apply Bayes rule:  

$$
\require{cancel}
p\left(\mathbf{s}_{t} | \mathbf{v}_{1:t}\right) = \frac{ \overbrace{p \left( \mathbf{v}_{t} \mid \mathbf{s}_{t}, \cancel{\mathbf{v}_{1:t-1}} \right )}^{\mathbf{v}_t ~ \text{only dep. on} ~ \mathbf{s}_t} p\left( \mathbf{s}_{t} \mid \mathbf{v}_{1:t-1} \right ) }{p\left( \mathbf{v}_t \mid \mathbf{v}_{1:t-1} \right )} = \frac{  \color{green}{g}\left( \mathbf{v}_{t} \mid \mathbf{s}_{t} \right ) p\left( \mathbf{s}_{t} \mid \mathbf{v}_{1:t-1} \right ) }{p\left( \mathbf{v}_t \mid \mathbf{v}_{1:t-1} \right )} \tag{3}\label{eq3}
$$

If this equation is confusing, think of the previous measurements $$\mathbf{v}_{1:t-1}$$ as just a "context", that is always on the conditioning side, a required "input" to all densities involved, with Bayes rule being applied to $$\mathbf{s}_{t}$$ and $$\mathbf{v}_{t}$$.
We know the current measurements only depends on the state, therefore $$p \left( \mathbf{v}_{t} \mid \mathbf{s}_{t}, \mathbf{v}_{1:t-1} \right ) = p \left( \mathbf{v}_{t} \mid \mathbf{s}_{t} \right ) = \color{green}{g}( \mathbf{v}_{t} \mid \mathbf{s}_{t} )$$, and only the right term in the numerator is left to compute. This term is a marginal of $$ \mathbf{s}_t$$, which means we have to integrate out anything else. If we were doing this very naively, each time we would integrate out all previous states, but by caching results a.k.a. Dynamic Programming, we only need to marginalize the previous state: 

$$
  p\left( \mathbf{s}_{t} \mid \mathbf{v}_{1:t-1} \right ) = \int p\left( \mathbf{s}_{t}, \mathbf{s}_{t-1} \mid \mathbf{v}_{1:t-1} \right ) \mathrm{d}\mathbf{s}_{t-1}
$$

Continuing, we split the joint with the product rule and exploit remember that the states are independent of previous measurements:

$$
\begin{equation}\begin{aligned}
  &= \int p\left( \mathbf{s}_{t} \mid  \mathbf{s}_{t-1}, \cancel{\mathbf{v}_{1:t-1}} \right ) p(\mathbf{s}_{t-1} \mid \mathbf{v}_{1:t-1}) \mathrm{d}\mathbf{s}_{t-1} \\
  &= \int p\left( \mathbf{s}_{t} \mid  \mathbf{s}_{t-1} \right ) p(\mathbf{s}_{t-1} \mid \mathbf{v}_{1:t-1}) \mathrm{d}\mathbf{s}_{t-1} \\
  &= \int \color{blue}{f}\left( \mathbf{s}_{t} \mid  \mathbf{s}_{t-1} \right ) p(\mathbf{s}_{t-1} \mid \mathbf{v}_{1:t-1}) \mathrm{d}\mathbf{s}_{t-1}
\end{aligned}\end{equation}\tag{4}\label{eq4}$$


And we are done, if you notice that the right side term in the integral is the filtering distribution at $$t-1$$, which we have already computed. 
In the literature names are given to the step that requires computing $$ p\left( \mathbf{s}_{t} \mid \mathbf{v}_{1:t-1} \right )$$ called *prediction*, because it's our belief on $$ \mathbf{s}_{t}$$ before observing the currrent measurement, and *correction* is the name given to the step $$ p\left(\mathbf{s}_{t} | \mathbf{v}_{1:t}\right) \propto \color{green}{g}\left( \mathbf{v}_{t} \mid \mathbf{s}_{t} \right ) \cdot p\left( \mathbf{s}_{t} \mid \mathbf{v}_{1:t-1} \right )$$, because we "correct" our prediction by taking into account the measurement. 

In a LDS, all computations have closed form solutions, and this algorithm instantiates into the *Kalman Filter*. For discrete  valued random variables, if the dimensionalities are small we can also do exact computations and the label this time is *Forward-Backward* algorithm for HMMs. 
When variables are non-Gaussian and/or transition/observation densities are nonlinear function of their inputs, we have to perform approximate inference. 
By far the most popular method is to use Monte Carlo approximations, and more specifically importance sampling. When we use importance sampling to approximate the filtering distribution, this is called *particle filtering*. 

### Recursive formulations <a name="sub2"></a>

Note that most particle filtering methods, which we will describe later, actually do not compute the filtering distribution with the correction and prediction steps. Instead, they express the filtering distribution by finding recursive relationships. Consider the sequential estimation of a different distribution to the filtering, namely: 

$$\begin{equation}\begin{aligned}
p(\mathbf{s}_{1:t} \mid \mathbf{v}_{1:t}) \propto p(\mathbf{s}_{1:t}, \mathbf{v}_{1:t}) 
\end{aligned}\end{equation}\tag{5}\label{eq5}$$

Let's take its unnormalized version for simplicity. Applying Bayes' rule gives the following recursive relationship:

$$\begin{equation}\begin{aligned}
p(\mathbf{s}_{1:t}, \mathbf{v}_{1:t}) = p(\mathbf{s}_{1:t-1}, \mathbf{v}_{1:t-1}) \color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t}) 
\end{aligned}\end{equation}\tag{6}\label{eq6}$$

If you can't see why this holds, consider this simple example/subcase: 

$$\begin{equation}\begin{aligned}
p(\mathbf{s}_{1}, \mathbf{s}_{2}, \mathbf{v}_{1}, \mathbf{v}_{2}) = p(\mathbf{s}_{1}, \mathbf{v}_{1}) \color{blue}{f}(\mathbf{s}_{2} \mid \mathbf{s}_{1}) \color{green}{g}(\mathbf{v}_{2} \mid \mathbf{s}_{2})
\end{aligned}\end{equation}$$

Hopefully this convinces you that \eqref{eq6} is true. Then, let's return to the task of sequentially estimating $$p(\mathbf{s}_{1:t} \mid \mathbf{v}_{1:t})$$: 


$$\begin{equation}\begin{aligned}
p(\mathbf{s}_{1:t} \mid \mathbf{v}_{1:t}) &= \frac{p(\mathbf{s}_{1:t}, \mathbf{v}_{1:t})}{p(\mathbf{v}_{1:t})} \\
&= \frac{p(\mathbf{s}_{1:t-1}, \mathbf{v}_{1:t-1}) \color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{1:t})} \\ 
&= \frac{p(\mathbf{s}_{1:t-1}, \mathbf{v}_{1:t-1})}{p(\mathbf{v}_{1:t-1})} \frac{\color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})} \\
&= p(\mathbf{s}_{1:t-1} \mid \mathbf{v}_{1:t-1}) \frac{\color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})}
\end{aligned}\end{equation}\tag{7}\label{eq7}$$

Now that we've gone through all this, we can get our real target, the filtering distribution, by simple marginalization of the expression we just found: 

$$\begin{equation}\begin{aligned}
p(\mathbf{s}_{t} \mid \mathbf{v}_{1:t}) &= \int p(\mathbf{s}_{1:t} \mid \mathbf{v}_{1:t}) \mathrm{d} \mathbf{s}_{1:t-1} \\
&= \int p(\mathbf{s}_{1:t-1} \mid \mathbf{v}_{1:t-1}) \frac{\color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})} \mathrm{d} \mathbf{s}_{1:t-1}\\
&= \frac{\color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})} \int p(\mathbf{s}_{1:\color{red}{t-1}} \mid \mathbf{v}_{1:t-1}) \color{blue}{f}(\mathbf{s}_{t} \mid \mathbf{s}_{t-1}) \mathrm{d} \mathbf{s}_{1:t-1} \\
&= \frac{\color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})} \overbrace{\int p(\mathbf{s}_{1:\color{red}{t}} \mid \mathbf{v}_{1:t-1}) \mathrm{d} \mathbf{s}_{1:t-1}}^{= p(\mathbf{s}_{t} \mid \mathbf{v}_{1:t-1}) ~ \text{by marginalization}} \\
&= \frac{p(\mathbf{s}_{t} \mid \mathbf{v}_{1:t-1}) \color{green}{g}(\mathbf{v}_{t} \mid \mathbf{s}_{t})}{p(\mathbf{v}_{t} \mid \mathbf{v}_{1:t-1})} 
\end{aligned}\end{equation}\tag{8}\label{eq8}$$

And 

## Particle filtering <a name="paragraph1"></a> 

In real-world settings, we need approximate inference. Specifically in the filtering/sequential Bayes literature, importance sampling based methods are more popular than deterministic approximations such as Laplace's method, Variational Bayes and Expectation Propagation. 
Recall that the Monte Carlo method is a general tool to approximate integrals, expectations, probabilities with random samples: 

$$
\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}] = \int \mathbf{x} p(\mathbf{x}) \mathrm{d}\mathbf{x} \approx \frac{1}{N} \sum_{n=1}^{N} \mathbf{x}_{n} \qquad \mathbf{x}_n \sim p(\mathbf{x})
\tag{9}\label{eq9}$$

Monte Carlo approximations of this kind are very appealing since unbiased and consistent, and it is easy to show that the variance of the estimate is $$ \mathcal{O}(n^{-1})$$ *regardless* of the dimensionality of the vector $$\mathbf{x}$$. Another simple idea that we will use extensively in particle filtering is that these samples can not only be used to approximate integrals with respect to the target distribution $$p(\mathbf{x})$$, but also to approximate the target itself:

$$
p(\mathbf{x}) \approx \frac{1}{N}\sum_{n=1}^{N} \delta_{\mathbf{x}}(\mathbf{x}_{n})
$$

Where $$  \delta_{\mathbf{x}}(\mathbf{x}_{n})$$ is the Dirac delta mass evaluated at point $$\mathbf{x}_{n}$$. 
Often it is not possible to sample from the distribution of interest. Therefore we can use importance sampling, which is a technique based on the following simple observation:

$$\begin{equation}\begin{aligned}
\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}] &= \int \mathbf{x} \cdot p(\mathbf{x}) \mathrm{d}\mathbf{x} \\
&= \int \frac{\mathbf{x} \cdot p(\mathbf{x})}{q(\mathbf{x})} \cdot q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&= \mathbb{E}_{q(\mathbf{x})} \left [ \mathbf{x} \cdot \frac{p(\mathbf{x})}{q(\mathbf{x})} \right ]
\end{aligned}\end{equation}\tag{10}\label{eq10}$$

Under certain conditions, namely that $$ \mathbf{x} \cdot p(\mathbf{x}) > 0 \Rightarrow q(\mathbf{x}) > 0$$, we have rewritten the expectation under a distribution of choice $$q(\mathbf{x})$$c alled *proposal* which we can sample from. Note that it is not possible to have $$ q(\mathbf{x}) = 0$$, as we will never sample any $$\mathbf{x}_{i}$$ from $$q$$ such that this holds. 
Let's return in the context of Bayesian inference, where we have a target posterior distribution $$ \pi(\mathbf{x}) = p(\mathbf{x} \mid \mathcal{D}) $$ where $$ \mathcal{D}$$ is any observed data. Then also let $$ \tilde{\pi}(\mathbf{x}) = p(\mathbf{x}, \mathcal{D)) $$ and consider an integral of some function of $$ \mathbf{x}$$ under the posterior: 

$$\begin{equation}\begin{aligned}
\mathcal{I} = \mathbb{E}_{\pi(\mathbf{x})}[f(\mathbf{x}] = \int f(x) \pi(\mathbf{x}) 
\end{aligned}\end{equation}\tag{11}\label{eq11}$$

Note that we can estimate this integral in two main ways with importance sampling: the former which assumes that we know the normalizing constant of the posterior $$ \pi(\mathbf{x})$$, and the latter estimates the normalizing constant too by importance sampling, with the same set of samples. Let's examine the latter option first:

$$\begin{equation}\begin{aligned}
\mathbb{E}_{\pi} \left [ f(\mathbf{x}) \right ] &= \int f(\mathbf{x}) \pi(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&= \int  f(\mathbf{x})\frac{\pi(\mathbf{x})}{q(\mathbf{x})} q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&= \int  f(\mathbf{x})\frac{p(\mathbf{x}, \mathcal{D})}{p(\mathcal{D})q(\mathbf{x})} q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&=  \frac{1}{p(\mathcal{D})}\int  f(\mathbf{x})\frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})} q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&=  \frac{1}{\int p(\mathbf{x}, \mathcal{D}) \mathrm{d} \mathbf{x}}\int  f(\mathbf{x})\frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})} q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&=  \frac{1}{\int \frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})}  q(\mathbf{x}) \mathrm{d} \mathbf{x}}\int  f(\mathbf{x})\frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})} q(\mathbf{x}) \mathrm{d} \mathbf{x} \\
&= \frac{1}{\mathbb{E}_{q}\left [ \frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})} \right ]}
\cdot \mathbb{E}_{q}\left [ f(\mathbf{x}) \frac{p(\mathbf{x}, \mathcal{D})}{q(\mathbf{x})} \right ] \\
&\approx \frac{1}{\cancel{\frac{1}{N}}\sum_{n=1}^{N} \frac{p(\mathbf{x}_i , \mathcal{D})}{q(\mathbf{x}_i)}}
\cdot ~ \cancel{\frac{1}{N}} \sum_{n=1}^{N} f(\mathbf{x}_i) \frac{p(\mathbf{x}_i, \mathcal{D})}{q(\mathbf{x}_i)} 
\end{aligned}\end{equation}\tag{12}\label{eq12}$$


## The Auxiliary Particle Filter and its interpretations <a name="paragraph2"></a>

One of the most prevalent intepretations of the APF is that the structure of the "optimal" importance distribution $$q()$$

## The Improved Particle Filter through the Multiple Importance Sampling Interpretation <a name="paragraph3"></a>

In this interpretation of the APF, the motivation is as follows. In SMC , if we had acces to the optimal proposal $$ p(x_t \mid y_t, x_{t-1}) $$ (which minimizes the variance of the weights) , then the multiplicative update to the weights becomes an expression that *does not* depend on the current state $$x_t$$. Therefore, it is possible to *interchange the order* of the sampling and resampling steps. Intuitively, this yields a better approximation of the distribution as it provides a greater
number of distinct particles to approximate the target. This is an example of a general principle: resampling,
if it is to be applied in a particular iteration, should be performed before, rather than after, any operation
that doesn’t influence the importance weights in order to minimise the loss of information. However, in general the incremental importance weights do depend upon the new states and this straightforward change of order becomes impossible. *In a sense, this interchange of sampling and resampling produces an algorithm in which information from the next observation is used to determine which particles should survive resampling at a given time*. It is desirable to
find methods for making use of this future information in a more general setting, so that we can obtain the
same advantage in situations in which it is not possible to make use of the optimal proposal distribution. 

This is how we motivate wanting to use $$y_{t+1}$$ in APF. It is shown that APF can be reinterpreted as a standard SMC algorithm that uses a different "target" distribution. I'll explain what is meant by "target". 
In standard filtering , we use the following recursion to update the weights: 

$$\begin{equation}\begin{aligned}
 W_{n}\left(x_{1: t}\right) &=\frac{\gamma_{t}\left(x_{1: t}\right)}{q_{t}\left(x_{1: t}\right)} \\ &=\frac{1}{q_{t-1}\left(x_{1: t-1}\right)} \frac{\gamma_{t-1}\left(x_{1: t-1}\right)}{\gamma_{t-1}\left(x_{1: t-1}\right)} \frac{\gamma_{t}\left(x_{1: t}\right)}{q_{t}\left(x_{t} | x_{1: t-1}\right)} \\ &=\frac{\gamma_{t-1}\left(x_{1: t-1}\right)}{q_{t-1}\left(x_{1: t-1}\right)} \frac{\gamma_{t}\left(x_{1: t}\right)}{\gamma_{t-1}\left(x_{1: t-1}\right) q_{t}\left(x_{t} | x_{1: t-1}\right)} \\
 &= W_{t-1}(x_{1:t-1}) \cdot \frac{\gamma_{t}(x_{1:t})}{\gamma_{t-1}(x_{1:t-1}) q_{t}(x_{t}\mid x_{1:t-1})}
\end{aligned}\end{equation}\tag{13}\label{eq13}$$

In SMC for filtering i.e. the sequential estimation of $$\left \{ p( x_{1:t} \mid y_{1:t}) \right \}_{t>=1} $$, the "target" $$\gamma_{t}(x_{1:t}) :=  p(x_{1:t}, y_{1:t})$$ i.e. the unnormalized posterior. We are also making use of a proposal with the structure : 

$$\begin{equation}\begin{aligned}
q_{t}\left(x_{1: t}\right)=q_{t-1}\left(x_{1: t-1}\right) q_{t}\left(x_{t} | x_{1: t-1}\right)
\end{aligned}\end{equation}\tag{14}\label{eq14}$$

so that the trick of expressing the new weight in \eqref{eq13} in terms of the previous weights is possible. Hence, the only choice goes into $$q_{t}\left(x_{t} \mid x_{1: t-1}\right)$$. When the target is $$\gamma_{t}(x_{1:t}) = p(x_{1:t}, y_{1:t}) = p(x_{1:t-1}, y_{1:t-1}) \color{blue}{f}(x_t \mid x_{t-1}) \cdot \color{green}{g}(y_t \mid x_t)$$ , then the weight update in \eqref{eq13} becomes : 


$$\begin{equation}\begin{aligned}
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\gamma_{t}(x_{1:t})}{\gamma_{t-1}(x_{1:t-1}) q_{t}(x_{t}\mid x_{1:t-1})} \\
&=  W_{t-1}(x_{1:t-1}) \cdot \frac{\color{blue}{f}(x_{t}\mid x_{t-1}) \color{green}{g}(y_t \mid x_t) \overbrace{p(x_{1:t-1}, y_{1:t-1})}^{\cancel{\gamma_{t-1}(x_{1:t-1})}}}{\cancel{\gamma_{t-1}(x_{1:t-1})} q(x_t \mid x_{1:t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\color{blue}{f}(x_{t}\mid x_{t-1}) \color{green}{g}(y_t \mid x_t)}{q(x_t \mid x_{1:t-1})}
\end{aligned}\end{equation}\tag{15}\label{eq15}$$$$

In the APF we want to make use of $$ y_{t+1}$$ and thus set the target $$\gamma_{t}(x_{1:t}) = p(x_{1:t}, y_{1:\color{red}{t+1}})$$. By expanding this we can express it as:

$$\begin{equation}\begin{aligned}
\gamma_{t}(x_{1:t}) := p(x_{1:t} , y_{\color{red}{t+1}}) &= \int p(x_{1:t+1} , y_{1:t+1}) \mathrm{d} x_{t+1} \\
&= \int p(x_{1:t} , y_{1:t}) \cdot {\color{blue}{f}}(x_{t+1} \mid x_{t}) \cdot {\color{green}{g}}(y_{t+1} \mid x_{t+1}) \mathrm{d} x_{t+1} \\
&=  p(x_{1:t} , y_{1:t}) \int {\color{blue}{f}}(x_{t+1} \mid x_{t}) \cdot {\color{green}{g}}(y_{t+1} \mid x_{t+1}) \mathrm{d} x_{t+1} \\
&=  p(x_{1:t} , y_{1:t}) \cdot \underbrace{p(y_{t+1} \mid x_{t})}_{"predictive~likelihood"}
\end{aligned}\end{equation}\tag{16}\label{eq16}$$

Where if the predictive likelihood $$p(y_t \mid x_{t-1})$$ is replaced with an approximation $$\hat{p}(y_t \mid x_{t-1})$$ if not known analytically. 
Therefore in APF the weight update becomes: 

$$\begin{equation}\begin{aligned}
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\gamma_{t}(x_{1:t})}{\gamma_{t-1}(x_{1:t-1}) q_{t}(x_{t}\mid x_{1:t-1})} \\
&=  W_{t-1}(x_{1:t-1}) \cdot  \frac{p(x_{1:t}, y_{1:t}) \hat{p}(y_{t+1} \mid x_t)}{p(x_{1:t-1}, y_{1:t-1}) \hat{p}(y_t \mid x_{t-1}) q(x_t \mid x_{1:t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\cancel{p(x_{1:t-1}, y_{1:t-1})} \color{blue}{f}(x_{t} \mid x_{t-1}) \color{green}{g}(y_t \mid x_t) \hat{p}(y_{t+1} \mid x_t)}{\cancel{p(x_{1:t-1}, y_{1:t-1})} \hat{p}(y_t \mid x_{t-1}) q(x_t \mid x_{1:t-1})} 
\end{aligned}\end{equation}\tag{17}\label{eq17}$$

since we have a different target $$ \gamma_{t}(x_{1:t})$$ to the unnormalized filtering $$p(x_{1:t}, y_{1:t})$$, the algorithm actually sequentially estimates $$\left \{ \hat{p}(x_{1:t} \mid y_{1:t+1}) = p(x_{1:t} \mid y_{1:t}) \hat{p}(y_{t+1} \mid x_t) \right \}_{t \geq 1} $$ , therefore we need to use importance sampling to obtain $$\left \{ p(x_{1:t} \mid y_{1:t}) \propto p(x_{1:t}, y_{1:t}) \right \}_{t \geq 1}$$, with the following importance distribution:

$$\begin{equation}\begin{aligned}
\hat{p}(x_{1:t-1} , y_{1:t}) q_{t}(x_{t} \mid y_t, x_{t-1})
\end{aligned}\end{equation}\tag{18}\label{eq18}$$

So that the weight update becomes (what we actually want divided by proposal):

$$\begin{equation}\begin{aligned}
&= W_{t-1}(x_{1:t-1}) \cdot \frac{p(x_{1:t} \mid y_{1:t})}{\hat{p}(x_{1:t-1} \mid y_{1:t}) q_{t}(x){t} \mid y_t, x_{t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot  \frac{p(x_{1:t}, y_{1:t})}{\hat{p}(x_{1:t-1} , y_{1:t}) q_{t}(x){t} \mid y_t, x_{t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\frac{p(x_{1:t}, y_{1:t})}{\cancel{Z}}}{\frac{p(x_{1:t-1}, y_{1:t-1})}{\cancel{Z}} \hat{p}(y_{t} \mid x_{t-1}) q_{t}(x_t \mid y_t, x_{t-1})} \\
&=  W_{t-1}(x_{1:t-1}) \cdot \frac{\cancel{p(x_{1:t-1}, y_{1:t-1})}f(x_{t}\mid x_{t-1}) g(y_t \mid x_t)}{\cancel{p(x_{1:t-1}, y_{1:t-1})}\hat{p}(y_{t} \mid x_{t-1}) q_{t}(x_t \mid y_t, x_{t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot \frac{f(x_{t}\mid x_{t-1}) g(y_t \mid x_t)}{\hat{p}(y_{t} \mid x_{t-1}) q_{t}(x_t \mid y_t, x_{t-1})}
\end{aligned}\end{equation}\tag{19}\label{eq19}$$ 

Selecting $$q_t(x_t \mid x_{1:t-1}) =  p(x_t \mid y_t, x_{t-1})$$ and $$\hat{p}(y_t \mid x_{t-1}) = p(y_t \mid x_{t-1})$$
leads to the so called "perfect adaptation" . 
Setting the approximation to the predictive likelihood (the true is equal to $$ p(y_t \mid x_{t-1}) = \int {\color{blue}{f}}(x_{t} \mid x_{t-1}) \cdot {\color{green}{g}}(y_{t} \mid x_{t}) \mathrm{d} x_{t} $$) to $$\hat{p}(y_{t} \mid x_{t-1}) = g(y_t \mid \mu(x_{t-1})) $$ where $$ \mu(x_{t-1})$$ is some likely value is common. For example , if we choose as approximation to the predictive likelihood : $$\hat{p}(y_{t} \mid x_{t-1}) = g(y_t \mid \mu_{t-1}) $$  where $$\mu_{t-1}$$ is the mean of $$ f(x_t \mid x_{t-1}) $$ *and* we also choose $$ q_t(x_{t} \mid y_t, x_{t-1}) = f(x_t \mid x_{t-1})$$ , then we recover as special case the popular version of the APF weights: 

$$\begin{equation}\begin{aligned}
&= W_{t-1}(x_{1:t-1}) \cdot \frac{f(x_{t}\mid x_{t-1}) g(y_t \mid x_t)}{\hat{p}(y_{t} \mid x_{t-1}) q_{t}(x_t \mid y_t, x_{t-1})} \\
&= W_{t-1}(x_{1:t-1}) \cdot \frac{\cancel{f(x_{t}\mid x_{t-1})} g(y_t \mid x_t)}{g(y_t \mid \mu_{t-1}) \cancel{f(x_{t}\mid x_{t-1})}}
\end{aligned}\end{equation}\tag{20}\label{eq20}$$ 
 
## References 
1. Elvira, V., Martino, L., Bugallo, M.F. and Djurić, P.M., 2018, September. In search for improved auxiliary particle filters. In 2018 26th European Signal Processing Conference (EUSIPCO) (pp. 1637-1641). IEEE.
2. Doucet, A. and Johansen, A.M., 2009. A tutorial on particle filtering and smoothing: Fifteen years later. Handbook of nonlinear filtering, 12(656-704), p.3.
3. Särkkä, S., 2013. Bayesian filtering and smoothing (Vol. 3). Cambridge University Press.




